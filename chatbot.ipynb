{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnnuPrasad/Chatbot_bengali/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avI6aJ-Rzm_2"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "import json\n",
        "\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kKM-wLFO0eJ6"
      },
      "outputs": [],
      "source": [
        "# Sample dataset (you can replace this with your Bengali dataset)\n",
        "data_1 = open('data.json').read()\n",
        "\n",
        "data = json.loads(data_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wywcImFw1Wty"
      },
      "outputs": [],
      "source": [
        "!pip install SpeechRecognition"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "dSOksUzSBMDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx3OmEKc0g63"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pydub import AudioSegment\n",
        "from nltk.corpus import stopwords\n",
        "import speech_recognition as sr\n",
        "\n",
        "\n",
        "for filename in os.listdir(\"/content/audios\"):\n",
        "    if not filename.endswith(\".wav\"):\n",
        "        print(f\"File '{filename}' is not in WAV format.\")\n",
        "\n",
        "# Initialize the recognizer\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "# Define a function to transcribe audio files\n",
        "import os\n",
        "from pydub import AudioSegment\n",
        "import speech_recognition as sr\n",
        "\n",
        "# Initialize the recognizer\n",
        "recognizer = sr.Recognizer()\n",
        "\n",
        "def transcribe_audio(audio_folder):\n",
        "    transcribed_texts = []  # List to store transcribed texts\n",
        "    for filename in os.listdir(audio_folder):\n",
        "        if filename.endswith(\".wav\"):  # Check if the file is a WAV file\n",
        "            audio_file_path = os.path.join(audio_folder, filename)\n",
        "            with sr.AudioFile(audio_file_path) as source:\n",
        "                audio_data = recognizer.record(source)\n",
        "    return transcribed_texts\n",
        "\n",
        "\n",
        "audio_folder_path = \"/content/audios\"\n",
        "transcribed_texts = transcribe_audio(audio_folder_path)\n",
        "\n",
        "\n",
        "# Replace sample_data with the transcribed text\n",
        "data_1 = open(\"data.json\").read()\n",
        "\n",
        "data = json.loads(data_1)\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# Initialize lemmatizer and stopwords\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('bengali'))\n",
        "\n",
        "\n",
        "# # Tokenize, lemmatize, and remove stopwords\n",
        "# tokenized_data = []\n",
        "# for sentence in data:\n",
        "#     lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize tokens\n",
        "#     filtered_tokens = [token for token in lemmatized_tokens if token not in stop_words]  # Remove stop words\n",
        "#     tokenized_data.append(filtered_tokens)\n",
        "\n",
        "# # Remove duplicate words\n",
        "# for i in range(len(tokenized_data)):\n",
        "#     tokenized_data[i] = list(set(tokenized_data[i]))\n",
        "\n",
        "# print(tokenized_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install classes"
      ],
      "metadata": {
        "id": "IYD9f0yPOO1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bnlp"
      ],
      "metadata": {
        "id": "ftpCdfPEPu9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Define your classes for Bengali language\n",
        "classes = [\"politics\", \"sports\", \"entertainment\", \"technology\", \"health\", \"education\"]\n",
        "\n",
        "# Sample documents (replace with your actual data)\n",
        "documents = [\n",
        "    (\"খেলাধুলা সম্পর্কে সংবাদ\", \"sports\"),\n",
        "    (\"প্রযুক্তি নিয়ে আলোচনা\", \"technology\"),\n",
        "    (\"সরকারের নীতিমালা সম্পর্কে বিবৃতি\", \"politics\"),\n",
        "    (\"বিনোদনের প্রচ্ছদ\", \"entertainment\"),\n",
        "    (\"স্বাস্থ্য সংক্রান্ত প্রতিবেদন\", \"health\"),\n",
        "    (\"শিক্ষা ও শিক্ষার্থীরা\", \"education\")\n",
        "]\n",
        "\n",
        "# Initialize an empty list for training data\n",
        "training = []\n",
        "\n",
        "# Create an empty array of zeros for output representation\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# Collect all unique words from documents\n",
        "words = set()\n",
        "for doc in documents:\n",
        "    words.update(word_tokenize(doc[0]))\n",
        "\n",
        "# Iterate over each document for processing\n",
        "for doc in documents:\n",
        "    bag = []\n",
        "\n",
        "    # Tokenize words in the document\n",
        "    pattern_words = word_tokenize(doc[0])\n",
        "    pattern_words = [word.lower() for word in pattern_words]\n",
        "\n",
        "    # Create bag of words representation\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # Create output row for classification\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    # Append bag of words and output row to training data\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# Shuffle training data\n",
        "random.shuffle(training)\n",
        "\n",
        "# Separate features and labels\n",
        "train_x = np.array([sample[0] for sample in training])\n",
        "train_y = np.array([sample[1] for sample in training])\n",
        "\n",
        "# Print shapes for verification\n",
        "print(\"Shape of train_x:\", train_x.shape)\n",
        "print(\"Shape of train_y:\", train_y.shape)\n"
      ],
      "metadata": {
        "id": "1JghTittNRYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.optimizers.legacy import SGD as LegacySGD\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "\n",
        "# Add layers to the model\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation='softmax'))\n",
        "\n",
        "# Use the legacy optimizer\n",
        "sgd = LegacySGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)\n",
        "\n",
        "# Save the model\n",
        "model.save('chatbot_model.h5')\n"
      ],
      "metadata": {
        "id": "oE9WvXR4M2Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(sentence, model):\n",
        "\n",
        "    p = bow(sentence, words, show_details=False)\n",
        "\n",
        "    res = model.predict(np.array([p]))[0]\n",
        "\n",
        "    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]\n",
        "\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return_list = []\n",
        "\n",
        "    for r in results:\n",
        "\n",
        "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
        "\n",
        "    return return_list"
      ],
      "metadata": {
        "id": "X105D8H7NYr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyQt5"
      ],
      "metadata": {
        "id": "2haflh-rSBUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tk"
      ],
      "metadata": {
        "id": "3j5UL2hBW5Bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-tk\n"
      ],
      "metadata": {
        "id": "Qs3hHRe3cip4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y xvfb"
      ],
      "metadata": {
        "id": "Z41xuvCmYIvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualdisplay\n"
      ],
      "metadata": {
        "id": "JA_UJIteYRBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export DISPLAY=:0"
      ],
      "metadata": {
        "id": "LHdrcfP4YUdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "PEKynctoeELs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter notebook --port 8888 --no-browser &"
      ],
      "metadata": {
        "id": "o0L3zddZYe7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GUI with Tkinter\n",
        "\n",
        "import tkinter as tk\n",
        "from tkinter import *\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "# Function to send message and get response\n",
        "def chatbot_response(msg):\n",
        "    # Here you would implement the logic to generate the chatbot's response based on the user's message\n",
        "    # For demonstration, let's just return a static response\n",
        "    return \"This is a static response from the chatbot.\"\n",
        "\n",
        "def send():\n",
        "    msg = EntryBox.get(\"1.0\",'end-1c').strip()\n",
        "    EntryBox.delete(\"0.0\",END)\n",
        "    if msg != '':\n",
        "        ChatLog.config(state=NORMAL)\n",
        "        ChatLog.insert(END, \"You: \" + msg + '\\n\\n')\n",
        "        ChatLog.config(foreground=\"#442265\", font=(\"Verdana\", 12))\n",
        "        res = chatbot_response(msg)\n",
        "        ChatLog.insert(END, \"Bot: \" + res + '\\n\\n')\n",
        "        ChatLog.config(state=DISABLED)\n",
        "        ChatLog.yview(END)\n",
        "\n",
        "# Create a virtual display\n",
        "display = Display(visible=0, size=(800, 600))\n",
        "display.start()\n",
        "\n",
        "# GUI setup\n",
        "root = tk.Tk()\n",
        "root.title(\"Chatbot\")\n",
        "root.geometry(\"400x500\")\n",
        "root.resizable(width=FALSE, height=FALSE)\n",
        "\n",
        "ChatLog = Text(root, bd=0, bg=\"white\", height=\"8\", width=\"50\", font=\"Arial\")\n",
        "ChatLog.config(state=DISABLED)\n",
        "\n",
        "scrollbar = Scrollbar(root, command=ChatLog.yview, cursor=\"heart\")\n",
        "ChatLog['yscrollcommand'] = scrollbar.set\n",
        "\n",
        "SendButton = Button(root, font=(\"Verdana\",12,'bold'), text=\"Send\", width=\"12\", height=5, bd=0, bg=\"#32de97\", activebackground=\"#3c9d9b\",fg='#ffffff', command= send )\n",
        "\n",
        "EntryBox = Text(root, bd=0, bg=\"white\",width=\"29\", height=\"5\", font=\"Arial\")\n",
        "\n",
        "scrollbar.place(x=376,y=6, height=386)\n",
        "ChatLog.place(x=6,y=6, height=386, width=370)\n",
        "EntryBox.place(x=128, y=401, height=90, width=265)\n",
        "SendButton.place(x=6, y=401, height=90)\n",
        "\n",
        "root.mainloop()\n",
        "\n",
        "# Stop the virtual display\n",
        "display.stop()\n"
      ],
      "metadata": {
        "id": "nOZ6DKRENkYJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSc/PuolZHyn7nSR/VYcSS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}